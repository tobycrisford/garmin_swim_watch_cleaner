# Work in progress

A Garmin swim watch uses an accelerometer to detect when a swimmer has finished one length of the pool and started a new one. The length of the swimming pool can also be input manually into the watch, and this is used to calculate the total distance covered. However, the length detection is not perfect. Sometimes false length transitions are recorded, and it is also possible that a real transition might be missed.

I have some swim watch data from a swimmer (lets call her Alice) where it is clear that the total distance has been significantly overestimated, but it is not immediately obvious which length transitions are wrong and which are real. In this project I will use a Bayesian approach to estimate the number of lengths that Alice actually swam. I think that calculating the posterior distribution exactly will be too hard, so I'll use a Monte Carlo method to generate repeated random samples from it instead.

A particular challenge is that Alice had never timed her swims before, so does not have a very good idea of how long a typical length should take. She also switches between two different strokes in an irregular way, with one stroke faster than the other. This complicates the pattern further.


# Current notes

First iteration did well on simulated data, but not well in practice. On inspection, this seems to be at least partly because the watch has a minimum length it will record (about 10 seconds), and this has the effect that an additional kind of error is quite common: the time is recorded in the wrong place if a fake length transition is wrongly recorded <10 secs before a real one (which is then guaranteed to be missed).

I've updated the model to account for this effect, that a real length transition <X seconds after a recorded value is guaranteed to be missed. But there are two additional choices I can make here: (1) also impose a minimum 10 second length requirement on the actual lengths (not just recorded) by reducing the probability of <10 second lengths to 0, or (2) remain entirely agnostic about possible actual length times. If I go with option (1), I can't explore the space of possibilities as effectively. It seems to find it hard to discover that it is able to add in small adjustments to the length times if it is not able to create very short lengths first. But if I go with option (2), the simulation can often end up just placing loads of missed lengths shortly after the recorded lengths (now with little penalty), to find that a large number of very short lengths best explains the data.

Option (1) seems to be required, and I think some mistakes in my current implementation could be fixed and that would help it to still explore the solution space properly. E.g. at the moment, when a recording is labelled fake, the next random hop operates on the next length, so there is no opportunity on that pass to add in a potential missing length transition shortly after the fake one. This means the 'fake' label has to survive two iterations before we potentially boost its likelihood significantly by adding in the missing time afterwards (the missing time is also uniformly sampled from the next interval so often won't be in the right place for the transition to be a good one).

I think there are currently a few bugs in my order of iterations of this kind, which means that my invariant distribution is probably not actually equal to the posterior distribution I want to sample from, even though it seems to approximate it pretty well based on simulated data results. I think I should sort these bugs out to see if option (1) still fails to find good answers in a reasonable number of hops. Hopefully this will fix the issue.